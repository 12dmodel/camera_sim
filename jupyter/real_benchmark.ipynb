{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Populating the interactive namespace from numpy and matplotlib\n"
     ]
    }
   ],
   "source": [
    "%pylab inline\n",
    "import sys\n",
    "\n",
    "sys.path.insert(0, '..')\n",
    "\n",
    "import torch\n",
    "from torch.autograd import Variable\n",
    "from torch import FloatTensor\n",
    "from torch.utils.data import DataLoader\n",
    "import torch.utils.data\n",
    "from skimage import data\n",
    "import matplotlib.pyplot as plt\n",
    "import skimage\n",
    "import numpy as np\n",
    "import os\n",
    "from tqdm import tqdm\n",
    "import glob\n",
    "\n",
    "from utils.training_util import calculate_psnr, calculate_ssim, prep_for_vis\n",
    "from utils.visualize_util import transpose_table, write_table_of_images\n",
    "\n",
    "from utils.image_utils import center_crop_tensor\n",
    "from utils import HTML\n",
    "from models.model_utils import get_model\n",
    "from data_generation.pipeline import ImageDegradationPipeline\n",
    "from data_generation.constants import XYZ2sRGB, ProPhotoRGB2XYZ\n",
    "from data_generation.data_utils import random_crop\n",
    "\n",
    "\n",
    "torch.no_grad()\n",
    "\n",
    "\n",
    "def numpy2tensor(arr):\n",
    "    if len(arr.shape) < 3:\n",
    "        arr = np.expand_dims(arr, -1)\n",
    "    return FloatTensor(arr).permute(2, 0, 1).unsqueeze(0).float() / 255.0\n",
    "\n",
    "def tensor2numpy(t, idx=None):\n",
    "    t = torch.clamp(t, 0, 1)\n",
    "    if idx is None:\n",
    "        t = t[0, ...]\n",
    "    else:\n",
    "        t = t[idx, ...]\n",
    "    return (t.detach().permute(1, 2, 0).cpu().squeeze().numpy() * 255.0).astype('uint8')\n",
    "\n",
    "\n",
    "class RealCellphoneDSLRDataset(torch.utils.data.Dataset):\n",
    "\n",
    "    def __init__(self,\n",
    "                 clean_img_root,\n",
    "                 noisy_img_root,\n",
    "                 clean_img_ext='jpg',\n",
    "                 noisy_img_ext='npy',\n",
    "                 im_size=None,\n",
    "                 crop_type='center',\n",
    "                 clean_exp_adj=0.0,\n",
    "                 noisy_exp_adj=0.0,\n",
    "                 file_list = None,\n",
    "                 n_patch_per_img=1,\n",
    "                ):\n",
    "        \"\"\"\n",
    "            Turn on with_context by default, because extra stuff in the output dicts\n",
    "            doesn't matter, and we mostly do center crop anyway.\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "\n",
    "        self._DEGRADED_EXT = noisy_img_ext\n",
    "        self._TARGET_EXT = clean_img_ext\n",
    "        self._DEGRADED_DIR = noisy_img_root\n",
    "        self._TARGET_DIR = clean_img_root\n",
    "        \n",
    "        self.undo_srgb = ImageDegradationPipeline([\n",
    "            ('UndosRGBGamma', {}),\n",
    "        ])\n",
    "        \n",
    "        if file_list is None:\n",
    "            file_list = glob.glob(os.path.join(self._DEGRADED_DIR,\n",
    "                                               '*.' + self._DEGRADED_EXT))\n",
    "            file_list = [os.path.basename(f) for f in file_list]\n",
    "            file_list = [os.path.splitext(f)[0] for f in file_list]\n",
    "        self.file_list = sorted(file_list)\n",
    "        self.count = len(self.file_list)\n",
    "        assert self.count > 0\n",
    "        self.im_size = im_size\n",
    "        self.crop_type = crop_type\n",
    "        self.clean_exp_adj = ImageDegradationPipeline(\n",
    "                [\n",
    "                    ('ExposureAdjustment', {'nstops': clean_exp_adj}),\n",
    "                    ('PixelClip', {}),\n",
    "                ]\n",
    "        )\n",
    "        self.noisy_exp_adj = ImageDegradationPipeline(\n",
    "                [\n",
    "                    ('ExposureAdjustment', {'nstops': noisy_exp_adj}),\n",
    "                    ('PixelClip', {}),\n",
    "                ]\n",
    "        )\n",
    "        self.n_patch_per_img = n_patch_per_img\n",
    "        self.crop_windows = [None] * self.count\n",
    "        \n",
    "    def _npy_loader(self, path):\n",
    "        img = np.load(path).astype('float32')\n",
    "        img = FloatTensor(img).permute(2, 0, 1)\n",
    "        return img\n",
    "    \n",
    "    def _t7_loader(self, path):\n",
    "        img = torch.load(path)\n",
    "        return img\n",
    "    \n",
    "    def _jpg_loader(self, path):\n",
    "        img = skimage.io.imread(path).astype('float32') / 255.0\n",
    "        img = FloatTensor(img).permute(2, 0, 1).unsqueeze(0)\n",
    "        return self.undo_srgb(img).squeeze(0)\n",
    "\n",
    "    def _load_img(self, path):\n",
    "        _, ext = os.path.splitext(path)\n",
    "        if ext == \".jpg\" or ext == \".png\":\n",
    "            img = self._jpg_loader(path)\n",
    "        elif ext == \".npy\":\n",
    "            img = self._npy_loader(path)\n",
    "        elif ext == \".t7\":\n",
    "            img = self._t7_loader(path)\n",
    "        else:\n",
    "            raise ValueError(\"Unrecognized extension received: {}\".format(ext))\n",
    "        return img\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        img_idx = index % self.count\n",
    "        crop_idx = index // self.count\n",
    "        # Load Degraded Image\n",
    "        degraded_path = os.path.join(self._DEGRADED_DIR,\n",
    "                                     self.file_list[img_idx] + \\\n",
    "                                             '.' + \\\n",
    "                                             self._DEGRADED_EXT)\n",
    "        target_path = os.path.join(self._TARGET_DIR,\n",
    "                                   self.file_list[img_idx] + \\\n",
    "                                           '.' + \\\n",
    "                                           self._TARGET_EXT)\n",
    "\n",
    "        degraded = self._load_img(degraded_path)\n",
    "#         degraded += torch.randn_like(degraded) * 0.1\n",
    "        target = self._load_img(target_path)\n",
    "#         degraded = target + torch.randn_like(target) * 0.1\n",
    "        if self.im_size is not None:\n",
    "            im = torch.cat([degraded, target], 0)\n",
    "\n",
    "            crop_sz = self.im_size\n",
    "                \n",
    "            if self.crop_type == 'center':\n",
    "                im = center_crop_tensor(im.unsqueeze(0), crop_sz[0], crop_sz[1])[0]\n",
    "            elif self.crop_type == 'random':\n",
    "                if self.crop_windows[img_idx] is None:\n",
    "                    # randomize and memorize it.\n",
    "                    w, h = self.im_size\n",
    "                    tw = im.size(-1)\n",
    "                    th = im.size(-2)\n",
    "                    wn = []\n",
    "                    for c in range(self.n_patch_per_img):\n",
    "                        h0 = np.random.randint(th - h)\n",
    "                        w0 = np.random.randint(tw - w)\n",
    "                        h1 = h0 + h\n",
    "                        w1 = w0 + w\n",
    "                        wn.append((h0, w0, h1, w1))\n",
    "                    self.crop_windows[img_idx] = wn\n",
    "                h0, w0, h1, w1 = self.crop_windows[img_idx][crop_idx]\n",
    "                im = im[..., h0:h1, w0:w1]\n",
    "            else:\n",
    "                raise ValueError(\"Invalid crop type received: {}\".format(self.crop_type))\n",
    "            im = im.squeeze()\n",
    "            degraded, target = torch.split(im, int(im.size(0) / 2), dim=0)\n",
    "\n",
    "        degraded = self.noisy_exp_adj(degraded.unsqueeze(0)).squeeze(0)\n",
    "        target = self.clean_exp_adj(target.unsqueeze(0)).squeeze(0)\n",
    "        \n",
    "        data = {'degraded_img': degraded,\n",
    "                'original_img': target}\n",
    "        \n",
    " \n",
    "        return data\n",
    "                            \n",
    "    def __len__(self):\n",
    "        return self.count * self.n_patch_per_img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Set These Values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# base_html = PATH_TO_OUTPUT_HTML\n",
    "base_html = '/afs/csail.mit.edu/u/t/tiam/public_html/phone2dslr/sandbox'\n",
    "\n",
    "# Note that this clean images are averaged RAW and they are not properly tonemapped.\n",
    "\n",
    "clean_img_root = \"../samples/sample_test/clean\"\n",
    "noisy_img_root = \"../samples/sample_test/noisy\"\n",
    "file_list = None # a list of string or None if to detect automatically\n",
    "clean_ext = 't7'\n",
    "noisy_ext = 't7'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from configobj import ConfigObj\n",
    "from validate import Validator\n",
    "\n",
    "from utils.training_util import load_statedict_runtime\n",
    "\n",
    "\n",
    "def run_test(expname,\n",
    "             max_test_batch,\n",
    "             max_visual_batch,\n",
    "             test_data,\n",
    "             manual_seed=3,\n",
    "             iteration='best',\n",
    "             max_model_patch_size=None,\n",
    "             model_patch_buffer=None,\n",
    "            ):\n",
    "    # if get_visual is True, focus on just getting the images\n",
    "    # Load pytorch model\n",
    "    # Get Configs\n",
    "    config_file = '../denoiser_specs/{}.conf'.format(expname)\n",
    "    config_spec = '../denoiser_specs/configspec.conf'\n",
    "\n",
    "    configspec = ConfigObj(config_spec, raise_errors=True)\n",
    "    config = ConfigObj(config_file, configspec=configspec, raise_errors=True, file_error=True)\n",
    "    config.validate(Validator())\n",
    "    \n",
    "    # Build the model\n",
    "    use_gpu = torch.cuda.is_available()\n",
    "    model = get_model(config[\"architecture\"], max_model_patch_size, model_patch_buffer)\n",
    "    if use_gpu:\n",
    "        model = model.cuda()\n",
    "\n",
    "    # Load Checkpoint\n",
    "    checkpoint_dir = config[\"training\"][\"checkpoint_dir\"]\n",
    "    print(expname)\n",
    "    if iteration is not None:\n",
    "        state_dict, global_iter = load_statedict_runtime(checkpoint_dir, iteration)\n",
    "        model.load_state_dict(state_dict)\n",
    "        print('Iteration = ', global_iter)\n",
    "    else:\n",
    "        global_iter = 0\n",
    "        print('Not loading pretrained network')\n",
    "\n",
    "    # Prepare to save output\n",
    "    rows = []\n",
    "\n",
    "    # Fivek Dataset\n",
    "    # max_test_batch = float('inf')\n",
    "    n_vis = 4\n",
    "    batch_size = 1\n",
    "\n",
    "    torch.manual_seed(manual_seed)\n",
    "    test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=True, num_workers=1)\n",
    "    \n",
    "    configs_postprocess = [\n",
    "        ('PixelClip', {}),\n",
    "        ('sRGBGamma', {}),\n",
    "    ]\n",
    "\n",
    "    pipeline_postprocess = ImageDegradationPipeline(configs_postprocess)\n",
    "    \n",
    "    n = 0.0\n",
    "    psnr = 0.0\n",
    "    input_psnr = 0.0\n",
    "    ssim = 0.0\n",
    "    input_ssim = 0.0\n",
    "    model.eval()\n",
    "    n_test_batch = len(test_loader)\n",
    "    n_test_batch = min(n_test_batch, max_test_batch)\n",
    "    pbar = tqdm(total=n_test_batch, desc=\"Evaluating batches\")\n",
    "    input_imglist = []\n",
    "    gt_imglist = []\n",
    "    output_imglist = []\n",
    "#     model = model.train()\n",
    "    model = model.eval()\n",
    "    for iter, batch in enumerate(test_loader):\n",
    "        if iter > n_test_batch:\n",
    "            break\n",
    "        if use_gpu:\n",
    "            degraded_img = batch['degraded_img'].cuda()\n",
    "            target_img = batch['original_img'].cuda()\n",
    "            if 'context_img' in batch and use_context:\n",
    "                context_img = batch['context_img'].cuda()\n",
    "                extra_args = {'context_img': context_img}\n",
    "            else:\n",
    "                extra_args = {}\n",
    "        else:\n",
    "            degraded_img = batch['degraded_img']\n",
    "            target_img = batch['original_img']\n",
    "            if 'context_img' in batch and use_context:\n",
    "                context_img = batch['context_img']\n",
    "                extra_args = {'context_img': context_img}\n",
    "            else:\n",
    "                extra_args = {}\n",
    "\n",
    "        output_img = model(degraded_img, extra_args=extra_args)\n",
    "\n",
    "#         min_v = torch.min(output_img)\n",
    "#         max_v = torch.max(output_img)\n",
    "    #     output_img = (output_img - min_v) / (max_v - min_v)\n",
    "        exp = batch['vis_exposure'] if 'vis_exposure' in batch else None\n",
    "        psnr += calculate_psnr(output_img, target_img)\n",
    "        input_psnr += calculate_psnr(degraded_img, target_img)\n",
    "        ssim += calculate_ssim(output_img, target_img)\n",
    "        input_ssim += calculate_ssim(degraded_img, target_img)\n",
    "        n += 1.0\n",
    "        if max_visual_batch > iter:\n",
    "            d = pipeline_postprocess(degraded_img) * 255.0\n",
    "            t = pipeline_postprocess(target_img) * 255.0\n",
    "            o = pipeline_postprocess(output_img) * 255.0\n",
    "            d = torch.clamp(d, 0, 255)\n",
    "            t = torch.clamp(t, 0, 255)\n",
    "            o = torch.clamp(o, 0, 255)\n",
    "            d = d.cpu().permute(0, 2, 3, 1).data.numpy().astype('uint8')\n",
    "            t = t.cpu().permute(0, 2, 3, 1).data.numpy().astype('uint8')\n",
    "            o = o.cpu().permute(0, 2, 3, 1).data.numpy().astype('uint8')\n",
    "            # d, t, o = prep_for_vis(degraded_img, target_img, output_img, exp)\n",
    "            d = np.squeeze(d)\n",
    "            t = np.squeeze(t)\n",
    "            o = np.squeeze(o)\n",
    "            input_imglist.append(d)\n",
    "            gt_imglist.append(t)\n",
    "            output_imglist.append(o)\n",
    "        pbar.update(1)\n",
    "    pbar.close()\n",
    "    psnr /= n\n",
    "    input_psnr /= n\n",
    "    ssim /= n\n",
    "    input_ssim /= n\n",
    "    return input_imglist, \\\n",
    "           gt_imglist, \\\n",
    "           output_imglist, \\\n",
    "           psnr, input_psnr, \\\n",
    "           ssim, input_ssim, \\\n",
    "           global_iter\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Get Visual Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Experiments to compare\n",
    "\n",
    "expnames = [\n",
    "                ('full_dataset_n3net', 'latest', 'N3Net'),\n",
    "           ]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# S7_ISP\n",
    "\n",
    "url_name = \"n3net\"\n",
    "DATASET_NAME = \"tiam_iphone8\"\n",
    "\n",
    "crop_size = (200, 200)\n",
    "n_patch_per_img = 3\n",
    "\n",
    "test_data = RealCellphoneDSLRDataset(clean_img_root=clean_img_root,\n",
    "                                     noisy_img_root=noisy_img_root,\n",
    "                                     clean_img_ext=clean_ext,\n",
    "                                     noisy_img_ext=noisy_ext,\n",
    "                                     file_list=file_list,\n",
    "                                     im_size=crop_size,\n",
    "                                     n_patch_per_img=n_patch_per_img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using No Normalization\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\r",
      "Evaluating batches:   0%|          | 0/30 [00:00<?, ?it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "full_dataset_n3net\n",
      "Iteration =  62000\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Evaluating batches: 100%|██████████| 30/30 [00:21<00:00,  1.40it/s]\n",
      "/media/hdd5tb/tiam/bin/.virtualenvs/camsim/local/lib/python3.6/site-packages/skimage/io/_io.py:140: UserWarning: /afs/csail.mit.edu/u/t/tiam/public_html/phone2dslr/sandbox/tiam_iphone8_n3net/img0.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/media/hdd5tb/tiam/bin/.virtualenvs/camsim/local/lib/python3.6/site-packages/skimage/io/_io.py:140: UserWarning: /afs/csail.mit.edu/u/t/tiam/public_html/phone2dslr/sandbox/tiam_iphone8_n3net/img12.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/media/hdd5tb/tiam/bin/.virtualenvs/camsim/local/lib/python3.6/site-packages/skimage/io/_io.py:140: UserWarning: /afs/csail.mit.edu/u/t/tiam/public_html/phone2dslr/sandbox/tiam_iphone8_n3net/img27.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/media/hdd5tb/tiam/bin/.virtualenvs/camsim/local/lib/python3.6/site-packages/skimage/io/_io.py:140: UserWarning: /afs/csail.mit.edu/u/t/tiam/public_html/phone2dslr/sandbox/tiam_iphone8_n3net/img36.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/media/hdd5tb/tiam/bin/.virtualenvs/camsim/local/lib/python3.6/site-packages/skimage/io/_io.py:140: UserWarning: /afs/csail.mit.edu/u/t/tiam/public_html/phone2dslr/sandbox/tiam_iphone8_n3net/img39.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n",
      "/media/hdd5tb/tiam/bin/.virtualenvs/camsim/local/lib/python3.6/site-packages/skimage/io/_io.py:140: UserWarning: /afs/csail.mit.edu/u/t/tiam/public_html/phone2dslr/sandbox/tiam_iphone8_n3net/img57.png is a low contrast image\n",
      "  warn('%s is a low contrast image' % fname)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/afs/csail.mit.edu/u/t/tiam/public_html/phone2dslr/sandbox/tiam_iphone8_n3net/index.html\n",
      "Done\n"
     ]
    }
   ],
   "source": [
    "output_html = '{}/index.html'.format(DATASET_NAME + '_' + url_name)\n",
    "output_html = os.path.join(base_html, output_html)\n",
    "\n",
    "manual_seed = 5\n",
    "n_images = 25\n",
    "n_psnr_image = 500\n",
    "# n_psnr_image = 5\n",
    "table = []\n",
    "for i in range(len(expnames)):\n",
    "    expname = expnames[i][0]\n",
    "    iter_to_load = expnames[i][1]\n",
    "    disp_name = expnames[i][2]\n",
    "    input_img, gt_img, output_img, psnr, input_psnr, ssim, input_ssim, global_iter = run_test(expname,\n",
    "                                                                n_psnr_image,\n",
    "                                                                n_images,\n",
    "                                                                manual_seed=manual_seed,\n",
    "                                                                iteration=iter_to_load,\n",
    "                                                                test_data=test_data\n",
    "                                                                )\n",
    "    output_img.insert(0, '{} <br> PSNR = {:0.4g} dB <br> SSIM = {:0.4g} <br> ({} iter = {})'.format(disp_name,\n",
    "                                                                                psnr,\n",
    "                                                                                ssim,\n",
    "                                                                                str(iter_to_load),\n",
    "                                                                                global_iter))\n",
    "    table.append(output_img)\n",
    "input_img.insert(0, 'Noisy Images <br> PSNR = {:0.4g} dB <br> SSIM = {:0.4g}'.format(input_psnr, input_ssim))\n",
    "gt_img.insert(0, 'Clean Images')\n",
    "table.insert(0, input_img)\n",
    "table.insert(0, gt_img)\n",
    "\n",
    "table = transpose_table(table)\n",
    "write_table_of_images(output_html, table)\n",
    "print(output_html)\n",
    "print('Done')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(output_html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
